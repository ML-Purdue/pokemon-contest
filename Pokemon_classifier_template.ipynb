{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qoo-KMQYXRtE"
      },
      "source": [
        "# CELL 1 (do not change)\n",
        "\"\"\"import statements and boiler plate code\"\"\"\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf \n",
        "import tensorflow.keras as ks\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from typing import *\n",
        "import sys\n",
        "\n",
        "if sys.version_info >= (3, 8, 0):\n",
        "    from math import prod\n",
        "else:\n",
        "    # math.prod shim for Python 3.7 and older\n",
        "    def prod(iterable, *, start=1):\n",
        "        total = start\n",
        "        for element in iterable:\n",
        "            total *= element\n",
        "        return (total)\n",
        "\n",
        "def pretruberate(latent, model, bit, value):\n",
        "    if not isinstance(latent, np.ndarray):\n",
        "        latent = latent.numpy()\n",
        "    latent[0][bit] = step\n",
        "    return model.decode(latent)\n",
        "\n",
        "def pretruberate_range(latent, image, model, bit, vals):\n",
        "    fig, ax = plt.subplots(1, 11)\n",
        "    fig.set_size_inches(w = 15, h = 30)\n",
        "    for i, step in enumerate(vals):\n",
        "        rec = pretruberate(latent, model, bit, step)\n",
        "        ax[i].imshow(rec[0, :, :, 0])\n",
        "        ax[i].axis('off')\n",
        "    ax[10].imshow(image[0, :, :, 0])\n",
        "    ax[10].axis('off')\n",
        "    plt.show()\n",
        "    pass\n",
        "\n",
        "def get_sample(val, index, model, batch_size = None):\n",
        "    sample = val.skip(index).take(1)\n",
        "    for image, label in sample:\n",
        "        break\n",
        "    if batch_size is None:\n",
        "        latent = model.predict(image).numpy()\n",
        "    else:\n",
        "        latent = model.predict(image, batch_size = batch_size).numpy()\n",
        "    return image, latent\n",
        "\n",
        "\n",
        "def display(val, model, n = 10, x0 = 0, y0 = 1): \n",
        "    image, latent = get_sample(val, 11, model)\n",
        "    latent = model.predict(image).numpy()\n",
        "    latent = latent[0, :]\n",
        "    print(latent.shape)\n",
        "    norm = tfp.distributions.Normal(0,1)\n",
        "    vals_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
        "    vals_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
        "\n",
        "    fig, ax = plt.subplots(n, n)\n",
        "    fig.set_size_inches(w = 10, h = 10)\n",
        "    for i, x in enumerate(vals_x):\n",
        "        for j, y in enumerate(vals_y):\n",
        "            latent[x0], latent[y0] = x, y\n",
        "            rec = tf.sigmoid(model.decode(np.array([latent])))\n",
        "            ax[i, j].imshow(rec[0,:,:,0], cmap='gnuplot2')\n",
        "            ax[i, j].axis('off')\n",
        "    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
        "    plt.show()\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOXL2mRSb7rs"
      },
      "source": [
        "# CELL 2 preprocessing\n",
        "'''\n",
        "copy the pipeline above \n",
        "\n",
        "to the pipeline:\n",
        "    - add a random flip\n",
        "    - random hue, max alpha = 0.3\n",
        "    - random brightness, max alpha = 0.3\n",
        "    - random_contrast, min = 0.9, max = 1.1\n",
        "'''\n",
        "def preprocess(image: tf.Tensor, label: tf.Tensor, depth = 3, disp: bool = False):\n",
        "    shape = tf.shape(image)\n",
        "    image = tf.cast(image, dtype = tf.float32)\n",
        "    image = image/255\n",
        "\n",
        "    crop_1 = tf.cast(tf.shape(image)[0] * 22/28, dtype = tf.int32)\n",
        "    crop_2 = tf.cast(tf.shape(image)[1] * 26/28, dtype = tf.int32)\n",
        "\n",
        "    \"\"\"\n",
        "    use crop_1 and crop_2 as the range within which to generate a random number and random crop\n",
        "    \"\"\"\n",
        "    randint = NotImplemented\n",
        "\n",
        "    # new additions\n",
        "    if (tf.shape(image)[-1] == 3):\n",
        "        \"\"\"\n",
        "        add:\n",
        "            - random flip\n",
        "            - random hue, max alpha = 0.3\n",
        "            - random brightness, max alpha = 0.1\n",
        "            - random_contrast, min = 0.9, max = 1.1\n",
        "        \"\"\"\n",
        "\n",
        "    image = tf.image.resize(image, (shape[0], shape[1]))\n",
        "    label = tf.one_hot(label, depth)\n",
        "    if disp:\n",
        "        tf.print(\"shape: \", tf.shape(image))\n",
        "        tf.print(\"random value: \", randint)\n",
        "    return image, label\n",
        "\n",
        "\"\"\"script to load data set and print info\"\"\"\n",
        "(train, test), info = tfds.load(\"rock_paper_scissors\", split = ['train', 'test'], with_info=True, shuffle_files=True, as_supervised = True)\n",
        "print(info)\n",
        "\n",
        "\"\"\" Test \"\"\"\n",
        "train = train.map(lambda x, y: preprocess(x, y, disp = True)).batch(1)\n",
        "for i, (image, label) in enumerate(train):\n",
        "    if (tf.shape(image)[-1] != 3):\n",
        "        plt.imshow(image[0, ..., 0])\n",
        "    else:\n",
        "        plt.imshow(image[0, ...])\n",
        "    plt.show()\n",
        "    print(label)\n",
        "    if i > 10:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG9_JtWSdqG0"
      },
      "source": [
        "# CELL 3 build model\n",
        "model = ks.Sequential()\n",
        "model.add(ks.layers.Conv2D(\"\"\"fill this in\"\"\"))# inital feature extraction, get the most important information, so less filters is better, do a single down sample\n",
        "\"\"\"\n",
        "\n",
        "BUILD A Conv NN: \n",
        "\n",
        "ADD LAYERS TO BUILD A FULL CNN\n",
        "    - add layers to the sequential model using model.add(<the layer you choose>)\n",
        "JUST MAKE SURE YOU USE SOFTMAX AS THE LAST LAYER, the soft max layer is provided\n",
        "\n",
        "competition:\n",
        "    - the student with the lowest number of parameters and accuracy >= 90% on the validation set will win $15\n",
        "    - if multiple people have the same number of parameters, the money will be split between the winners, or it will go to the student with higher accuracy.\n",
        "\n",
        "\"\"\"\n",
        "model.add(ks.layers.Activation(activation = \"softmax\"))\n",
        "\n",
        "model.build(input_shape=(None, 300, 300, 3))\n",
        "model.summary()\n",
        "\n",
        "\"\"\"script to load data set and print info\"\"\"\n",
        "with open('pokemon.pkl', 'rb') as f:\n",
        "  df = pickle.load(f)                 # dataset dataframe\n",
        "target = df.pop('Type1')\n",
        "\n",
        "dataset = NotImplemented\n",
        "(train, test) = NotImplemented\n",
        "\n",
        "train = train.map(lambda x, y: preprocess(x, y, disp = False)).batch(40)\n",
        "test = test.map(lambda x, y: preprocess(x, y, disp = False)).batch(40)\n",
        "\n",
        "EPOCHS = 100\n",
        "optimizer = NotImplemented\n",
        "loss_fn = NotImplemented\n",
        "metrics_fn = ks.metrics.CategoricalAccuracy()\n",
        "\n",
        "\"\"\"script that will be used to test you model\"\"\"\n",
        "model.compile(optimizer=optimizer, loss = loss_fn, metrics)\n",
        "model.fit(train, validation_data = test, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}